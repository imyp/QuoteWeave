{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from huggingface_hub import HfApi, login, create_repo\n",
    "import json\n",
    "\n",
    "# # Login to Hugging Face\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with 4-bit quantization...\n",
      "Model loaded. Memory footprint: 0.12 GB\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "model_id = \"flan-t5-semantic-tagger/checkpoint-4638\"\n",
    "repo_name = \"flan-t5-semantic-tagger-small-4bit\"\n",
    "local_save_path = \"./my-4bit-model-3\"\n",
    "\n",
    "# 4-bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_quant_storage=torch.uint8,\n",
    ")\n",
    "\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded. Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository created: flan-t5-semantic-tagger-small-4bit\n",
      "Saving model locally...\n",
      "Pushing to Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef7a6b0a633491ea4bf01a02fb8f4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/118M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34daaaf6fbbd46b9bc9fd40f9a3e2a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b29d6d8634d490c8673e1b3d1744b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model successfully saved to: https://huggingface.co/flan-t5-semantic-tagger-small-4bit\n",
      "ðŸ“Š Memory footprint: 0.12 GB\n",
      "ðŸ’¾ Local copy saved to: ./my-4bit-model-3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create repository on Hugging Face Hub\n",
    "try:\n",
    "    create_repo(repo_name, exist_ok=True, private=False)\n",
    "    print(f\"Repository created: {repo_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository might already exist: {e}\")\n",
    "\n",
    "# Save model configuration with quantization info\n",
    "config_dict = model.config.to_dict()\n",
    "config_dict[\"quantization_config\"] = quantization_config.to_dict()\n",
    "\n",
    "# Save locally\n",
    "print(\"Saving model locally...\")\n",
    "model.save_pretrained(local_save_path, safe_serialization=True)\n",
    "tokenizer.save_pretrained(local_save_path)\n",
    "\n",
    "# Save custom config with quantization info\n",
    "with open(f\"{local_save_path}/config.json\", \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "\n",
    "# Create model card\n",
    "model_card_content = f\"\"\"---\n",
    "library_name: transformers\n",
    "license: apache-2.0\n",
    "base_model: {model_id}\n",
    "tags:\n",
    "- text2text-generation\n",
    "- t5\n",
    "- quantized\n",
    "- 4bit\n",
    "- bitsandbytes\n",
    "pipeline_tag: text2text-generation\n",
    "quantized: true\n",
    "---\n",
    "\n",
    "# {repo_name.split(\"/\")[-1]}\n",
    "\n",
    "This is a 4-bit quantized version of [{model_id}](https://huggingface.co/{model_id}) using BitsAndBytesConfig.\n",
    "\n",
    "## Quantization Details\n",
    "- **Bits**: 4-bit\n",
    "- **Compute dtype**: float16\n",
    "- **Double quantization**: True\n",
    "- **Quantization type**: nf4\n",
    "- **Memory usage**: ~{model.get_memory_footprint() / 1e9:.2f} GB\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"{repo_name}\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"{repo_name}\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "## Original Model\n",
    "Based on [{model_id}](https://huggingface.co/{model_id})\n",
    "\"\"\"\n",
    "\n",
    "# Create the repo on Hugging Face Hub\n",
    "create_repo(repo_name, exist_ok=True, private=False)\n",
    "\n",
    "# Save model card\n",
    "with open(f\"{local_save_path}/README.md\", \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "print(\"Pushing to Hugging Face Hub...\")\n",
    "model.push_to_hub(\n",
    "    repo_name,\n",
    "    commit_message=\"Upload 4-bit quantized model\",\n",
    "    safe_serialization=True,\n",
    "    create_pr=False,\n",
    ")\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "# # Upload the README separately to ensure it's included\n",
    "# hf_api = HfApi()\n",
    "# hf_api.upload_file(\n",
    "#     path_or_fileobj=f\"{local_save_path}/README.md\",\n",
    "#     path_in_repo=\"README.md\",\n",
    "#     repo_id=repo_name,\n",
    "#     repo_type=\"model\",\n",
    "# )\n",
    "\n",
    "print(f\"âœ… Model successfully saved to: https://huggingface.co/{repo_name}\")\n",
    "print(f\"ðŸ“Š Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "print(f\"ðŸ’¾ Local copy saved to: {local_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
