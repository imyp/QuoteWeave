{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from huggingface_hub import HfApi, login, create_repo\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"flan-t5-semantic-tagger-small\"\n",
    "repo_name = \"flan-t5-semantic-tagger-small-4bit\"\n",
    "pt_model_id = \"flan-t5-semantic-tagger-small-pt\"\n",
    "local_save_path = \"./my-4bit-model-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FlaxT5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some of the weights of FlaxT5ForConditionalGeneration were initialized in bfloat16 precision from the model checkpoint at flan-t5-semantic-tagger-small:\n",
      "[('decoder', 'block', '0', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('decoder', 'block', '0', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('decoder', 'block', '0', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('decoder', 'block', '0', 'layer', '0', 'SelfAttention', 'relative_attention_bias', 'embedding'), ('decoder', 'block', '0', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('decoder', 'block', '0', 'layer', '0', 'layer_norm', 'weight'), ('decoder', 'block', '0', 'layer', '1', 'EncDecAttention', 'k', 'kernel'), ('decoder', 'block', '0', 'layer', '1', 'EncDecAttention', 'o', 'kernel'), ('decoder', 'block', '0', 'layer', '1', 'EncDecAttention', 'q', 'kernel'), ('decoder', 'block', '0', 'layer', '1', 'EncDecAttention', 'v', 'kernel'), ('decoder', 'block', '0', 'layer', '1', 'layer_norm', 'weight'), ('decoder', 'block', '0', 'layer', '2', 'DenseReluDense', 'wi_0', 'kernel'), ('decoder', 'block', '0', 'layer', '2', 'DenseReluDense', 'wi_1', 'kernel'), ('decoder', 'block', '0', 'layer', '2', 'DenseReluDense', 'wo', 'kernel'), ('decoder', 'block', '0', 'layer', '2', 'layer_norm', 'weight'), ('decoder', 'block', '1', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('decoder', 'block', '1', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('decoder', 'block', '1', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('decoder', 'block', '1', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('decoder', 'block', '1', 'layer', '0', 'layer_norm', 'weight'), ('decoder', 'block', '1', 'layer', '1', 'EncDecAttention', 'k', 'kernel'), ('decoder', 'block', '1', 'layer', '1', 'EncDecAttention', 'o', 'kernel'), ('decoder', 'block', '1', 'layer', '1', 'EncDecAttention', 'q', 'kernel'), ('decoder', 'block', '1', 'layer', '1', 'EncDecAttention', 'v', 'kernel'), ('decoder', 'block', '1', 'layer', '1', 'layer_norm', 'weight'), ('decoder', 'block', '1', 'layer', '2', 'DenseReluDense', 'wi_0', 'kernel'), ('decoder', 'block', '1', 'layer', '2', 'DenseReluDense', 'wi_1', 'kernel'), ('decoder', 'block', '1', 'layer', '2', 'DenseReluDense', 'wo', 'kernel'), ('decoder', 'block', '1', 'layer', '2', 'layer_norm', 'weight'), ('decoder', 'block', '2', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('decoder', 'block', '2', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('decoder', 'block', '2', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('decoder', 'block', '2', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('decoder', 'block', '2', 'layer', '0', 'layer_norm', 'weight'), ('decoder', 'block', '2', 'layer', '1', 'EncDecAttention', 'k', 'kernel'), ('decoder', 'block', '2', 'layer', '1', 'EncDecAttention', 'o', 'kernel'), ('decoder', 'block', '2', 'layer', '1', 'EncDecAttention', 'q', 'kernel'), ('decoder', 'block', '2', 'layer', '1', 'EncDecAttention', 'v', 'kernel'), ('decoder', 'block', '2', 'layer', '1', 'layer_norm', 'weight'), ('decoder', 'block', '2', 'layer', '2', 'DenseReluDense', 'wi_0', 'kernel'), ('decoder', 'block', '2', 'layer', '2', 'DenseReluDense', 'wi_1', 'kernel'), ('decoder', 'block', '2', 'layer', '2', 'DenseReluDense', 'wo', 'kernel'), ('decoder', 'block', '2', 'layer', '2', 'layer_norm', 'weight'), ('decoder', 'block', '3', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('decoder', 'block', '3', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('decoder', 'block', '3', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('decoder', 'block', '3', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('decoder', 'block', '3', 'layer', '0', 'layer_norm', 'weight'), ('decoder', 'block', '3', 'layer', '1', 'EncDecAttention', 'k', 'kernel'), ('decoder', 'block', '3', 'layer', '1', 'EncDecAttention', 'o', 'kernel'), ('decoder', 'block', '3', 'layer', '1', 'EncDecAttention', 'q', 'kernel'), ('decoder', 'block', '3', 'layer', '1', 'EncDecAttention', 'v', 'kernel'), ('decoder', 'block', '3', 'layer', '1', 'layer_norm', 'weight'), ('decoder', 'block', '3', 'layer', '2', 'DenseReluDense', 'wi_0', 'kernel'), ('decoder', 'block', '3', 'layer', '2', 'DenseReluDense', 'wi_1', 'kernel'), ('decoder', 'block', '3', 'layer', '2', 'DenseReluDense', 'wo', 'kernel'), ('decoder', 'block', '3', 'layer', '2', 'layer_norm', 'weight'), ('decoder', 'block', '4', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('decoder', 'block', '4', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('decoder', 'block', '4', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('decoder', 'block', '4', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('decoder', 'block', '4', 'layer', '0', 'layer_norm', 'weight'), ('decoder', 'block', '4', 'layer', '1', 'EncDecAttention', 'k', 'kernel'), ('decoder', 'block', '4', 'layer', '1', 'EncDecAttention', 'o', 'kernel'), ('decoder', 'block', '4', 'layer', '1', 'EncDecAttention', 'q', 'kernel'), ('decoder', 'block', '4', 'layer', '1', 'EncDecAttention', 'v', 'kernel'), ('decoder', 'block', '4', 'layer', '1', 'layer_norm', 'weight'), ('decoder', 'block', '4', 'layer', '2', 'DenseReluDense', 'wi_0', 'kernel'), ('decoder', 'block', '4', 'layer', '2', 'DenseReluDense', 'wi_1', 'kernel'), ('decoder', 'block', '4', 'layer', '2', 'DenseReluDense', 'wo', 'kernel'), ('decoder', 'block', '4', 'layer', '2', 'layer_norm', 'weight'), ('decoder', 'block', '5', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('decoder', 'block', '5', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('decoder', 'block', '5', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('decoder', 'block', '5', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('decoder', 'block', '5', 'layer', '0', 'layer_norm', 'weight'), ('decoder', 'block', '5', 'layer', '1', 'EncDecAttention', 'k', 'kernel'), ('decoder', 'block', '5', 'layer', '1', 'EncDecAttention', 'o', 'kernel'), ('decoder', 'block', '5', 'layer', '1', 'EncDecAttention', 'q', 'kernel'), ('decoder', 'block', '5', 'layer', '1', 'EncDecAttention', 'v', 'kernel'), ('decoder', 'block', '5', 'layer', '1', 'layer_norm', 'weight'), ('decoder', 'block', '5', 'layer', '2', 'DenseReluDense', 'wi_0', 'kernel'), ('decoder', 'block', '5', 'layer', '2', 'DenseReluDense', 'wi_1', 'kernel'), ('decoder', 'block', '5', 'layer', '2', 'DenseReluDense', 'wo', 'kernel'), ('decoder', 'block', '5', 'layer', '2', 'layer_norm', 'weight'), ('decoder', 'block', '6', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('decoder', 'block', '6', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('decoder', 'block', '6', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('decoder', 'block', '6', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('decoder', 'block', '6', 'layer', '0', 'layer_norm', 'weight'), ('decoder', 'block', '6', 'layer', '1', 'EncDecAttention', 'k', 'kernel'), ('decoder', 'block', '6', 'layer', '1', 'EncDecAttention', 'o', 'kernel'), ('decoder', 'block', '6', 'layer', '1', 'EncDecAttention', 'q', 'kernel'), ('decoder', 'block', '6', 'layer', '1', 'EncDecAttention', 'v', 'kernel'), ('decoder', 'block', '6', 'layer', '1', 'layer_norm', 'weight'), ('decoder', 'block', '6', 'layer', '2', 'DenseReluDense', 'wi_0', 'kernel'), ('decoder', 'block', '6', 'layer', '2', 'DenseReluDense', 'wi_1', 'kernel'), ('decoder', 'block', '6', 'layer', '2', 'DenseReluDense', 'wo', 'kernel'), ('decoder', 'block', '6', 'layer', '2', 'layer_norm', 'weight'), ('decoder', 'block', '7', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('decoder', 'block', '7', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('decoder', 'block', '7', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('decoder', 'block', '7', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('decoder', 'block', '7', 'layer', '0', 'layer_norm', 'weight'), ('decoder', 'block', '7', 'layer', '1', 'EncDecAttention', 'k', 'kernel'), ('decoder', 'block', '7', 'layer', '1', 'EncDecAttention', 'o', 'kernel'), ('decoder', 'block', '7', 'layer', '1', 'EncDecAttention', 'q', 'kernel'), ('decoder', 'block', '7', 'layer', '1', 'EncDecAttention', 'v', 'kernel'), ('decoder', 'block', '7', 'layer', '1', 'layer_norm', 'weight'), ('decoder', 'block', '7', 'layer', '2', 'DenseReluDense', 'wi_0', 'kernel'), ('decoder', 'block', '7', 'layer', '2', 'DenseReluDense', 'wi_1', 'kernel'), ('decoder', 'block', '7', 'layer', '2', 'DenseReluDense', 'wo', 'kernel'), ('decoder', 'block', '7', 'layer', '2', 'layer_norm', 'weight'), ('decoder', 'final_layer_norm', 'weight'), ('encoder', 'block', '0', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('encoder', 'block', '0', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('encoder', 'block', '0', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('encoder', 'block', '0', 'layer', '0', 'SelfAttention', 'relative_attention_bias', 'embedding'), ('encoder', 'block', '0', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('encoder', 'block', '0', 'layer', '0', 'layer_norm', 'weight'), ('encoder', 'block', '0', 'layer', '1', 'DenseReluDense', 'wi_0', 'kernel'), ('encoder', 'block', '0', 'layer', '1', 'DenseReluDense', 'wi_1', 'kernel'), ('encoder', 'block', '0', 'layer', '1', 'DenseReluDense', 'wo', 'kernel'), ('encoder', 'block', '0', 'layer', '1', 'layer_norm', 'weight'), ('encoder', 'block', '1', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('encoder', 'block', '1', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('encoder', 'block', '1', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('encoder', 'block', '1', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('encoder', 'block', '1', 'layer', '0', 'layer_norm', 'weight'), ('encoder', 'block', '1', 'layer', '1', 'DenseReluDense', 'wi_0', 'kernel'), ('encoder', 'block', '1', 'layer', '1', 'DenseReluDense', 'wi_1', 'kernel'), ('encoder', 'block', '1', 'layer', '1', 'DenseReluDense', 'wo', 'kernel'), ('encoder', 'block', '1', 'layer', '1', 'layer_norm', 'weight'), ('encoder', 'block', '2', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('encoder', 'block', '2', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('encoder', 'block', '2', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('encoder', 'block', '2', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('encoder', 'block', '2', 'layer', '0', 'layer_norm', 'weight'), ('encoder', 'block', '2', 'layer', '1', 'DenseReluDense', 'wi_0', 'kernel'), ('encoder', 'block', '2', 'layer', '1', 'DenseReluDense', 'wi_1', 'kernel'), ('encoder', 'block', '2', 'layer', '1', 'DenseReluDense', 'wo', 'kernel'), ('encoder', 'block', '2', 'layer', '1', 'layer_norm', 'weight'), ('encoder', 'block', '3', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('encoder', 'block', '3', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('encoder', 'block', '3', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('encoder', 'block', '3', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('encoder', 'block', '3', 'layer', '0', 'layer_norm', 'weight'), ('encoder', 'block', '3', 'layer', '1', 'DenseReluDense', 'wi_0', 'kernel'), ('encoder', 'block', '3', 'layer', '1', 'DenseReluDense', 'wi_1', 'kernel'), ('encoder', 'block', '3', 'layer', '1', 'DenseReluDense', 'wo', 'kernel'), ('encoder', 'block', '3', 'layer', '1', 'layer_norm', 'weight'), ('encoder', 'block', '4', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('encoder', 'block', '4', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('encoder', 'block', '4', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('encoder', 'block', '4', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('encoder', 'block', '4', 'layer', '0', 'layer_norm', 'weight'), ('encoder', 'block', '4', 'layer', '1', 'DenseReluDense', 'wi_0', 'kernel'), ('encoder', 'block', '4', 'layer', '1', 'DenseReluDense', 'wi_1', 'kernel'), ('encoder', 'block', '4', 'layer', '1', 'DenseReluDense', 'wo', 'kernel'), ('encoder', 'block', '4', 'layer', '1', 'layer_norm', 'weight'), ('encoder', 'block', '5', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('encoder', 'block', '5', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('encoder', 'block', '5', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('encoder', 'block', '5', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('encoder', 'block', '5', 'layer', '0', 'layer_norm', 'weight'), ('encoder', 'block', '5', 'layer', '1', 'DenseReluDense', 'wi_0', 'kernel'), ('encoder', 'block', '5', 'layer', '1', 'DenseReluDense', 'wi_1', 'kernel'), ('encoder', 'block', '5', 'layer', '1', 'DenseReluDense', 'wo', 'kernel'), ('encoder', 'block', '5', 'layer', '1', 'layer_norm', 'weight'), ('encoder', 'block', '6', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('encoder', 'block', '6', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('encoder', 'block', '6', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('encoder', 'block', '6', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('encoder', 'block', '6', 'layer', '0', 'layer_norm', 'weight'), ('encoder', 'block', '6', 'layer', '1', 'DenseReluDense', 'wi_0', 'kernel'), ('encoder', 'block', '6', 'layer', '1', 'DenseReluDense', 'wi_1', 'kernel'), ('encoder', 'block', '6', 'layer', '1', 'DenseReluDense', 'wo', 'kernel'), ('encoder', 'block', '6', 'layer', '1', 'layer_norm', 'weight'), ('encoder', 'block', '7', 'layer', '0', 'SelfAttention', 'k', 'kernel'), ('encoder', 'block', '7', 'layer', '0', 'SelfAttention', 'o', 'kernel'), ('encoder', 'block', '7', 'layer', '0', 'SelfAttention', 'q', 'kernel'), ('encoder', 'block', '7', 'layer', '0', 'SelfAttention', 'v', 'kernel'), ('encoder', 'block', '7', 'layer', '0', 'layer_norm', 'weight'), ('encoder', 'block', '7', 'layer', '1', 'DenseReluDense', 'wi_0', 'kernel'), ('encoder', 'block', '7', 'layer', '1', 'DenseReluDense', 'wi_1', 'kernel'), ('encoder', 'block', '7', 'layer', '1', 'DenseReluDense', 'wo', 'kernel'), ('encoder', 'block', '7', 'layer', '1', 'layer_norm', 'weight'), ('encoder', 'final_layer_norm', 'weight'), ('lm_head', 'kernel'), ('shared', 'embedding')]\n",
      "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
     ]
    }
   ],
   "source": [
    "flax_model = FlaxT5ForConditionalGeneration.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found ``bfloat16`` weights in Flax model. Casting all ``bfloat16`` weights to ``float32`` before loading those in PyTorch model.\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for shared.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.1.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.1.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.1.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.0.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.1.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.1.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.1.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.1.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.1.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.1.layer.1.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.1.layer.1.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.1.layer.1.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.1.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.2.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.2.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.2.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.2.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.2.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.2.layer.1.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.2.layer.1.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.2.layer.1.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.2.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.3.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.3.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.3.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.3.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.3.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.3.layer.1.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.3.layer.1.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.3.layer.1.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.3.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.4.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.4.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.4.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.4.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.4.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.4.layer.1.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.4.layer.1.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.4.layer.1.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.4.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.5.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.5.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.5.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.5.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.5.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.5.layer.1.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.5.layer.1.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.5.layer.1.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.5.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.6.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.6.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.6.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.6.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.6.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.6.layer.1.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.6.layer.1.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.6.layer.1.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.6.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.7.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.7.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.7.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.7.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.7.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.7.layer.1.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.7.layer.1.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.7.layer.1.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.block.7.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for encoder.final_layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.1.EncDecAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.1.EncDecAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.1.EncDecAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.1.EncDecAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.2.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.2.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.2.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.0.layer.2.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.1.EncDecAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.1.EncDecAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.1.EncDecAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.1.EncDecAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.2.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.2.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.2.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.1.layer.2.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.1.EncDecAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.1.EncDecAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.1.EncDecAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.1.EncDecAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.2.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.2.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.2.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.2.layer.2.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.1.EncDecAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.1.EncDecAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.1.EncDecAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.1.EncDecAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.2.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.2.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.2.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.3.layer.2.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.1.EncDecAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.1.EncDecAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.1.EncDecAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.1.EncDecAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.2.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.2.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.2.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.4.layer.2.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.1.EncDecAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.1.EncDecAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.1.EncDecAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.1.EncDecAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.2.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.2.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.2.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.5.layer.2.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.1.EncDecAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.1.EncDecAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.1.EncDecAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.1.EncDecAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.2.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.2.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.2.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.6.layer.2.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.0.SelfAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.0.SelfAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.0.SelfAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.0.SelfAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.0.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.1.EncDecAttention.q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.1.EncDecAttention.k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.1.EncDecAttention.v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.1.EncDecAttention.o.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.1.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.2.DenseReluDense.wi_0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.2.DenseReluDense.wi_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.2.DenseReluDense.wo.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.block.7.layer.2.layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for decoder.final_layer_norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2409: UserWarning: for lm_head.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "All Flax model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the Flax model and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pt_model = T5ForConditionalGeneration.from_pretrained(model_id, from_flax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The weights trying to be saved contained shared tensors [{'decoder.block.1.layer.1.EncDecAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight'}, {'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight'}, {'encoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.final_layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight'}, {'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpt_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpt_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id)\n\u001b[32m      3\u001b[39m tokenizer.save_pretrained(pt_model_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3486\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3483\u001b[39m         error_names.append(\u001b[38;5;28mset\u001b[39m(shared_names))\n\u001b[32m   3485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_names) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3486\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   3487\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe weights trying to be saved contained shared tensors \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3488\u001b[39m         )\n\u001b[32m   3490\u001b[39m \u001b[38;5;66;03m# Shard the model if it is too big.\u001b[39;00m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _hf_peft_config_loaded:\n",
      "\u001b[31mRuntimeError\u001b[39m: The weights trying to be saved contained shared tensors [{'decoder.block.1.layer.1.EncDecAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight'}, {'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight'}, {'encoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.final_layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight'}, {'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing."
     ]
    }
   ],
   "source": [
    "pt_model.save_pretrained(pt_model_id, local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.save_pretrained(pt_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msafetensors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_file\n\u001b[32m      3\u001b[39m state_dict = torch.load(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpt_model_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/pytorch_model.bin\u001b[39m\u001b[33m\"\u001b[39m, map_location=device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpt_model_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/model.safetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/safetensors/torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/safetensors/torch.py:496\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshape\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/safetensors/torch.py:500\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    497\u001b[39m     k: {\n\u001b[32m    498\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m    499\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: v.shape,\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    501\u001b[39m     }\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    503\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/safetensors/torch.py:422\u001b[39m, in \u001b[36m_tobytes\u001b[39m\u001b[34m(tensor, name)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    415\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` which is not allowed. It either means you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms recommended to save\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m pack it before saving.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m     )\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor.device.type != \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "state_dict = torch.load(f\"{pt_model_id}/pytorch_model.bin\", map_location=device)\n",
    "save_file(state_dict, f\"{pt_model_id}/model.safetensors\", metadata={\"format\": \"pt\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with 4-bit quantization...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model with 4-bit quantization...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m tokenizer = AutoTokenizer.from_pretrained(pt_model_id)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpt_model_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4833\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4831\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m   4832\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m-> \u001b[39m\u001b[32m4833\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[32m   4852\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/repos/github/QuoteWeave/models/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:824\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[32m    822\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     _load_parameter_into_model(model, param_name, \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    827\u001b[39m     hf_quantizer.create_quantized_param(\n\u001b[32m    828\u001b[39m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001b[32m    829\u001b[39m     )\n",
      "\u001b[31mNotImplementedError\u001b[39m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "# 4-bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_quant_storage=torch.uint8,\n",
    ")\n",
    "\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_model_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    pt_model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
